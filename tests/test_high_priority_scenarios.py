"""
–¢–µ—Å—Ç—ã –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞:
1. Latency explosion –ø—Ä–∏ —Ä–æ—Å—Ç–µ –æ—á–µ—Ä–µ–¥–µ–π
2. Zombie tasks –ø—Ä–∏ –∫—Ä—ç—à–µ –≤–æ—Ä–∫–µ—Ä–∞
3. Data inconsistency –ø—Ä–∏ partial failure
4. Redis memory limits
"""

import pytest
import json
import asyncio
from datetime import datetime, timedelta
from redis.asyncio import Redis
from sqlalchemy.ext.asyncio import AsyncSession
from db.models import User, UserLinks
from repositories.base import BaseRepository
from misc.utils import worker_exsists, db_worker, marzban_worker
from unittest.mock import AsyncMock, patch, MagicMock
import time


# ============================================================================
# 1. LATENCY EXPLOSION - –¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
# ============================================================================

@pytest.mark.asyncio
async def test_worker_exists_performance_degradation(redis_client: Redis):
    """
    –¢–µ—Å—Ç: worker_exsists –¥–µ–≥—Ä–∞–¥–∏—Ä—É–µ—Ç –ø—Ä–∏ —Ä–æ—Å—Ç–µ –æ—á–µ—Ä–µ–¥–∏
    
    –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç O(N) —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    queue = "TEST_QUEUE"
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏
    for i in range(1000):
        task = {
            "user_id": 10000 + i,
            "type": "test"
        }
        await redis_client.lpush(queue, json.dumps(task, sort_keys=True)) #type: ignore
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–¥–∞—á—É –∫–æ—Ç–æ—Ä–æ–π –ù–ï–¢ –≤ –æ—á–µ—Ä–µ–¥–∏ (worst case)
    search_task = {
        "user_id": 99999,
        "type": "test"
    }
    
    # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
    start = time.time()
    exists = await worker_exsists(redis_client, queue, search_task)
    elapsed = time.time() - start
    
    print(f"\n‚è±Ô∏è  Time to check in 1000 items: {elapsed*1000:.2f}ms")
    
    assert exists is False
    # –ü—Ä–∏ 1000 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –æ–∂–∏–¥–∞–µ–º > 50ms
    assert elapsed > 0.05, f"Too fast: {elapsed*1000:.2f}ms - –≤–æ–∑–º–æ–∂–Ω–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ"


@pytest.mark.asyncio
async def test_worker_exists_scales_linearly(redis_client: Redis):
    """
    –¢–µ—Å—Ç: worker_exsists –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ O(N)
    
    –ü—Ä–∏ —É–¥–≤–æ–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –æ—á–µ—Ä–µ–¥–∏ –≤—Ä–µ–º—è —É–¥–≤–∞–∏–≤–∞–µ—Ç—Å—è
    """
    queue = "TEST_QUEUE"
    
    times = []
    
    for queue_size in [100, 500, 1000]:
        # –û—á–∏—â–∞–µ–º
        await redis_client.delete(queue)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º
        for i in range(queue_size):
            task = {"user_id": 10000 + i, "type": "test"}
            await redis_client.lpush(queue, json.dumps(task, sort_keys=True)) #type: ignore
        
        # –ò—â–µ–º –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–¥–∞—á—É
        search_task = {"user_id": 99999, "type": "test"}
        
        start = time.time()
        await worker_exsists(redis_client, queue, search_task)
        elapsed = time.time() - start
        
        times.append(elapsed)
        print(f"\nüìä Queue size {queue_size}: {elapsed*1000:.2f}ms")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–Ω–µ–π–Ω—ã–π —Ä–æ—Å—Ç (—Å –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å—é)
    # time(1000) / time(100) ‚âà 10
    ratio = times[2] / times[0]
    print(f"\nüìà Growth ratio (1000/100): {ratio:.2f}x")
    
    assert 5 < ratio < 15, f"Expected ~10x growth, got {ratio:.2f}x"


@pytest.mark.asyncio
async def test_concurrent_duplicate_checks_create_bottleneck(redis_client: Redis):
    """
    –¢–µ—Å—Ç: –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–∑–¥–∞—é—Ç bottleneck
    
    100 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤ –æ—á–µ—Ä–µ–¥–∏ –∏–∑ 1000 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    """
    queue = "TEST_QUEUE"
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å
    for i in range(1000):
        task = {"user_id": 10000 + i, "type": "test"}
        await redis_client.lpush(queue, json.dumps(task, sort_keys=True)) #type: ignore
    
    # 100 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
    async def check_task(user_id: int):
        task = {"user_id": user_id, "type": "test"}
        start = time.time()
        await worker_exsists(redis_client, queue, task)
        return time.time() - start
    
    start_total = time.time()
    tasks = [check_task(90000 + i) for i in range(100)]
    results = await asyncio.gather(*tasks)
    total_time = time.time() - start_total
    
    avg_time = sum(results) / len(results)
    
    print(f"\n‚è±Ô∏è  100 concurrent checks:")
    print(f"   Total time: {total_time:.2f}s")
    print(f"   Avg per check: {avg_time*1000:.2f}ms")
    print(f"   Throughput: {100/total_time:.2f} checks/sec")
    
    # –ü—Ä–∏ O(N) —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ throughput –±—É–¥–µ—Ç –Ω–∏–∑–∫–∏–π
    assert total_time > 1.0, "Should take >1 second for 100 checks in 1000 items"


# ============================================================================
# 2. ZOMBIE TASKS - –¢–µ—Å—Ç—ã –ø–æ—Ç–µ—Ä–∏ –∑–∞–¥–∞—á –ø—Ä–∏ –∫—Ä—ç—à–µ
# ============================================================================

@pytest.mark.asyncio
async def test_task_lost_on_worker_crash(redis_client: Redis, test_session: AsyncSession):
    """
    –¢–µ—Å—Ç: –∑–∞–¥–∞—á–∞ —Ç–µ—Ä—è–µ—Ç—Å—è –ø—Ä–∏ –∫—Ä—ç—à–µ –≤–æ—Ä–∫–µ—Ä–∞ –ø–æ—Å–ª–µ brpop
    
    –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ—Ä—é –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ processing queue
    """
    queue = "DB"
    
    task_data = {
        "model": "User",
        "type": "create",
        "user_id": 12345,
        "username": "test_user"
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É
    await redis_client.lpush(queue, json.dumps(task_data, sort_keys=True, default=str)) #type: ignore
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–¥–∞—á–∞ –≤ –æ—á–µ—Ä–µ–¥–∏
    queue_size = await redis_client.llen(queue) #type: ignore
    assert queue_size == 1
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á—É (–∏–º–∏—Ç–∞—Ü–∏—è brpop)
    result = await redis_client.brpop(queue, timeout=1) #type: ignore
    assert result is not None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–¥–∞—á–∞ –±–æ–ª—å—à–µ –Ω–µ –≤ –æ—á–µ—Ä–µ–¥–∏
    queue_size = await redis_client.llen(queue) #type: ignore
    assert queue_size == 0
    
    # ‚ùå –ò–º–∏—Ç–∞—Ü–∏—è –∫—Ä—ç—à–∞ - –∑–∞–¥–∞—á–∞ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∏ –Ω–µ –≤–µ—Ä–Ω—É–ª–∞—Å—å –≤ –æ—á–µ—Ä–µ–¥—å
    # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –∫—Ä—ç—à –ø—Ä–æ—Ü–µ—Å—Å–∞
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–¥–∞—á–∞ –ø–æ—Ç–µ—Ä—è–Ω–∞
    queue_size = await redis_client.llen(queue) #type: ignore
    assert queue_size == 0, "Task is lost forever"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ User –ù–ï —Å–æ–∑–¥–∞–Ω –≤ –ë–î
    repo = BaseRepository(session=test_session, model=User)
    user = await repo.get_one(user_id=12345)
    assert user is None, "User should not exist (task was lost)"
    
    print("\nüí• Task lost on crash - no recovery mechanism")


@pytest.mark.asyncio
async def test_processing_queue_prevents_task_loss(redis_client: Redis):
    """
    –¢–µ—Å—Ç: processing queue –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç –ø–æ—Ç–µ—Ä–∏ –∑–∞–¥–∞—á
    
    –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–∞–∫ RPOPLPUSH –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Ç–µ—Ä—é –¥–∞–Ω–Ω—ã—Ö
    """
    queue = "DB"
    processing_queue = "DB:processing"
    
    task_data = {
        "model": "User",
        "type": "create",
        "user_id": 12345
    }
    task_json = json.dumps(task_data, sort_keys=True, default=str)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É
    await redis_client.lpush(queue, task_json) #type: ignore
    
    # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º RPOPLPUSH –≤–º–µ—Å—Ç–æ BRPOP
    message = await redis_client.brpoplpush(queue, processing_queue, timeout=1) #type: ignore
    assert message is not None
    
    # –ó–∞–¥–∞—á–∞ –ø–µ—Ä–µ–º–µ—Å—Ç–∏–ª–∞—Å—å –∏–∑ queue –≤ processing_queue
    main_queue_size = await redis_client.llen(queue) #type: ignore
    processing_queue_size = await redis_client.llen(processing_queue) #type: ignore
    
    assert main_queue_size == 0, "Task moved from main queue"
    assert processing_queue_size == 1, "Task now in processing queue"
    
    # ‚ùå –ò–º–∏—Ç–∞—Ü–∏—è –∫—Ä—ç—à–∞
    # –ó–∞–¥–∞—á–∞ –æ—Å—Ç–∞–ª–∞—Å—å –≤ processing_queue - –º–æ–∂–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å!
    
    # Recovery: –ø–µ—Ä–µ–º–µ—â–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ
    recovered = await redis_client.rpoplpush(processing_queue, queue) #type: ignore
    assert recovered == message
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    main_queue_size = await redis_client.llen(queue) #type: ignore
    processing_queue_size = await redis_client.llen(processing_queue) #type: ignore
    
    assert main_queue_size == 1, "Task recovered to main queue"
    assert processing_queue_size == 0, "Processing queue is clean"
    
    print("\n‚úÖ Task recovered after crash using processing queue")


@pytest.mark.asyncio
async def test_zombie_task_detection_and_recovery(redis_client: Redis):
    """
    –¢–µ—Å—Ç: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ TTL
    """
    queue = "DB"
    processing_queue = "DB:processing"
    
    task_json = json.dumps({"model": "User", "type": "create", "user_id": 12345})
    
    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ processing
    await redis_client.lpush(queue, task_json) #type: ignore
    await redis_client.brpoplpush(queue, processing_queue, timeout=1) #type: ignore
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º TTL –º–∞—Ä–∫–µ—Ä (–∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —Ç–µ—Å—Ç–∞)
    task_id = f"{processing_queue}:{task_json}"
    await redis_client.setex(task_id, 2, "1")  # 2 —Å–µ–∫—É–Ω–¥—ã
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–∞—Ä–∫–µ—Ä —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    exists = await redis_client.exists(task_id)
    assert exists == 1
    
    # –ñ–¥—ë–º –∏—Å—Ç–µ—á–µ–Ω–∏—è TTL
    await asyncio.sleep(3)
    
    # TTL –∏—Å—Ç—ë–∫ - –∑–∞–¥–∞—á–∞ "–∑–∞–≤–∏—Å–ª–∞"
    exists = await redis_client.exists(task_id)
    assert exists == 0, "TTL expired - task is zombie"
    
    # Cleanup worker –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç
    tasks_in_processing = await redis_client.lrange(processing_queue, 0, -1) #type: ignore
    
    for task in tasks_in_processing:
        task_marker = f"{processing_queue}:{task}"
        marker_exists = await redis_client.exists(task_marker)
        
        if not marker_exists:
            # –ó–∞–≤–∏—Å—à–∞—è –∑–∞–¥–∞—á–∞ - –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
            await redis_client.lrem(processing_queue, 1, task) #type: ignore
            await redis_client.lpush(queue, task) #type: ignore
            print(f"\n‚ôªÔ∏è Recovered zombie task")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    main_queue_size = await redis_client.llen(queue) #type: ignore
    processing_queue_size = await redis_client.llen(processing_queue) #type: ignore
    
    assert main_queue_size == 1, "Zombie task recovered"
    assert processing_queue_size == 0, "Processing queue cleaned"


@pytest.mark.asyncio
async def test_multiple_worker_crashes_preserve_tasks(redis_client: Redis):
    """
    –¢–µ—Å—Ç: –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫—Ä—ç—à–µ–π –≤–æ—Ä–∫–µ—Ä–æ–≤ –Ω–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø–æ—Ç–µ—Ä–µ –∑–∞–¥–∞—á
    """
    queue = "DB"
    processing_queue = "DB:processing"
    
    # –î–æ–±–∞–≤–ª—è–µ–º 10 –∑–∞–¥–∞—á
    for i in range(10):
        task = {"model": "User", "type": "create", "user_id": 10000 + i}
        await redis_client.lpush(queue, json.dumps(task, sort_keys=True)) #type: ignore
    
    # –í–æ—Ä–∫–µ—Ä 1: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç 3 –∑–∞–¥–∞—á–∏ –∏ –∫—Ä–∞—à–∏—Ç—Å—è
    for _ in range(3):
        await redis_client.brpoplpush(queue, processing_queue, timeout=1) #type: ignore
    
    # –í–æ—Ä–∫–µ—Ä 2: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç 2 –∑–∞–¥–∞—á–∏ –∏ –∫—Ä–∞—à–∏—Ç—Å—è
    for _ in range(2):
        await redis_client.brpoplpush(queue, processing_queue, timeout=1) #type: ignore
    
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ –∫—Ä—ç—à–µ–π
    main_queue_size = await redis_client.llen(queue) #type: ignore
    processing_queue_size = await redis_client.llen(processing_queue) #type: ignore
    
    print(f"\nüìä After crashes:")
    print(f"   Main queue: {main_queue_size}")
    print(f"   Processing queue: {processing_queue_size}")
    
    assert main_queue_size == 5, "5 tasks not yet processed"
    assert processing_queue_size == 5, "5 tasks stuck in processing"
    
    # Recovery: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –∏–∑ processing –≤ main
    while True:
        task = await redis_client.rpoplpush(processing_queue, queue) #type: ignore
        if not task:
            break
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    main_queue_size = await redis_client.llen(queue) #type: ignore
    processing_queue_size = await redis_client.llen(processing_queue) #type: ignore
    
    assert main_queue_size == 10, "All 10 tasks recovered"
    assert processing_queue_size == 0, "Processing queue empty"
    
    print(f"‚úÖ All {main_queue_size} tasks recovered - zero data loss")


# ============================================================================
# 3. PARTIAL FAILURES - –¢–µ—Å—Ç—ã –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
# ============================================================================

@pytest.mark.asyncio
async def test_partial_failure_creates_inconsistency(
    redis_client: Redis,
    test_session: AsyncSession,
    monkeypatch
):
    """
    –¢–µ—Å—Ç: —á–∞—Å—Ç–∏—á–Ω—ã–π —Å–±–æ–π —Å–æ–∑–¥–∞—ë—Ç –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É User –∏ UserLinks
    
    User —Å–æ–∑–¥–∞–Ω, –Ω–æ UserLinks –Ω–µ—Ç ‚Üí subscription_url –ø–æ—Ç–µ—Ä—è–Ω
    """
    user_id = 12345
    
    # Mock Marzban API
    mock_response = {
        'subscription_url': 'https://dns1.example.com/sub/test',
        'expire': int((datetime.now() + timedelta(days=30)).timestamp())
    }
    
    # –°—á—ë—Ç—á–∏–∫ lpush –≤—ã–∑–æ–≤–æ–≤
    lpush_count = 0
    original_lpush = redis_client.lpush
    
    async def failing_lpush(*args, **kwargs):
        nonlocal lpush_count
        lpush_count += 1
        
        # –í—Ç–æ—Ä–æ–π lpush (UserLinks) –ø–∞–¥–∞–µ—Ç
        if lpush_count == 2:
            raise ConnectionError("Redis connection lost")
        
        return await original_lpush(*args, **kwargs) #type: ignore
    
    monkeypatch.setattr(redis_client, "lpush", failing_lpush)
    
    with patch('misc.utils.MarzbanClient') as MockClient:
        mock_instance = MagicMock()
        mock_instance.__aenter__.return_value = mock_instance
        mock_instance.__aexit__.return_value = None
        mock_instance.create = AsyncMock(return_value=mock_response)
        MockClient.return_value = mock_instance
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É
        task_data = {
            "type": "create",
            "user_id": user_id,
            "expire": mock_response['expire']
        }
        
        await redis_client.lpush("MARZBAN", json.dumps(task_data, sort_keys=True, default=str)) #type: ignore
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º - –¥–æ–ª–∂–µ–Ω —É–ø–∞—Å—Ç—å –Ω–∞ –≤—Ç–æ—Ä–æ–º lpush
        with pytest.raises(ConnectionError):
            await marzban_worker(redis_client)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–µ—Ä–≤—ã–π lpush (User) –ø—Ä–æ—à—ë–ª
    db_queue_size = await redis_client.llen("DB") #type: ignore
    assert db_queue_size == 1, "Only User task made it to queue"
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º User
    await db_worker(redis_client, test_session, process_once=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
    user_repo = BaseRepository(session=test_session, model=User)
    links_repo = BaseRepository(session=test_session, model=UserLinks)
    
    user = await user_repo.get_one(user_id=user_id)
    links = await links_repo.get_one(user_id=user_id)
    
    assert user is not None, "User created"
    assert links is None, "‚ùå UserLinks NOT created - subscription_url lost!"
    
    print("\nüí• Data inconsistency: User exists but UserLinks missing")
    print(f"   User: {user.user_id}")
    print(f"   Links: None")
    print(f"   subscription_url: LOST")


@pytest.mark.asyncio
async def test_transactional_outbox_prevents_inconsistency(redis_client: Redis):
    """
    –¢–µ—Å—Ç: transactional outbox –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Ç–µ—Ä—é –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ partial failure
    
    –í—Å–µ –∑–∞–¥–∞—á–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∞—Ç–æ–º–∞—Ä–Ω–æ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
    """
    transaction_id = "tx_12345"
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
    transaction = {
        "id": transaction_id,
        "marzban_result": {"subscription_url": "https://example.com"},
        "tasks": [
            {"model": "User", "type": "create", "user_id": 12345},
            {"model": "UserLinks", "type": "create", "user_id": 12345}
        ]
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –∞—Ç–æ–º–∞—Ä–Ω–æ
    tx_key = f"transaction:{transaction_id}"
    await redis_client.setex(
        tx_key,
        3600,
        json.dumps(transaction, default=str)
    )
    
    # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–¥–∞—á–∏ (–º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å –Ω–∞ –≤—Ç–æ—Ä–æ–π)
    sent_count = 0
    try:
        for task in transaction["tasks"]:
            await redis_client.lpush("DB", json.dumps(task, default=str)) #type: ignore
            sent_count += 1
            
            if sent_count == 1:
                # –ò–º–∏—Ç–∞—Ü–∏—è –∫—Ä—ç—à–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π –∑–∞–¥–∞—á–∏
                raise ConnectionError("Crash after first lpush")
    except ConnectionError:
        pass
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –∑–∞–¥–∞—á–∞
    queue_size = await redis_client.llen("DB") #type: ignore
    assert queue_size == 1, "Only 1 task sent before crash"
    
    # ‚úÖ –ù–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ - –º–æ–∂–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å!
    tx_data = await redis_client.get(tx_key)
    assert tx_data is not None, "Transaction preserved"
    
    saved_tx = json.loads(tx_data)
    assert len(saved_tx["tasks"]) == 2, "All tasks recorded"
    
    # Recovery: –ø—Ä–æ–≤–µ—Ä—è–µ–º committed —Å—Ç–∞—Ç—É—Å
    committed = await redis_client.exists(f"{tx_key}:committed")
    
    if not committed:
        # –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –Ω–µ –∑–∞–∫–æ–º–º–∏—á–µ–Ω–∞ - –ø–æ–≤—Ç–æ—Ä—è–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É –í–°–ï–• –∑–∞–¥–∞—á
        print(f"\n‚ôªÔ∏è Recovering incomplete transaction {transaction_id}")
        
        for task in saved_tx["tasks"]:
            await redis_client.lpush("DB", json.dumps(task, default=str)) #type: ignore
        
        # –ö–æ–º–º–∏—Ç–∏–º
        await redis_client.setex(f"{tx_key}:committed", 3600, "1")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    # –ü–µ—Ä–≤–∞—è –∑–∞–¥–∞—á–∞ –±—ã–ª–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –¥–≤–∞–∂–¥—ã, –Ω–æ —ç—Ç–æ OK (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å)
    queue_size = await redis_client.llen("DB") #type: ignore
    assert queue_size >= 2, "All tasks in queue after recovery"
    
    print(f"‚úÖ Transaction recovered: {queue_size} tasks in queue")


@pytest.mark.asyncio
async def test_marzban_success_but_db_tasks_lost(
    redis_client: Redis,
    test_session: AsyncSession
):
    """
    –¢–µ—Å—Ç: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–æ–∑–¥–∞–Ω –≤ Marzban, –Ω–æ DB –∑–∞–¥–∞—á–∏ –ø–æ—Ç–µ—Ä—è–Ω—ã
    
    Worst case: –¥–µ–Ω—å–≥–∏ —Å–ø–∏—Å–∞–Ω—ã, Marzban —Å–æ–∑–¥–∞–Ω, –Ω–æ –ë–î –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞
    """
    user_id = 12345
    
    mock_response = {
        'subscription_url': 'https://dns1.example.com/sub/test',
        'expire': int((datetime.now() + timedelta(days=30)).timestamp())
    }
    
    with patch('misc.utils.MarzbanClient') as MockClient:
        mock_instance = MagicMock()
        mock_instance.__aenter__.return_value = mock_instance
        mock_instance.__aexit__.return_value = None
        mock_instance.create = AsyncMock(return_value=mock_response)
        MockClient.return_value = mock_instance
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É
        task_data = {
            "type": "create",
            "user_id": user_id,
            "expire": mock_response['expire']
        }
        
        await redis_client.lpush("MARZBAN", json.dumps(task_data, sort_keys=True, default=str)) #type: ignore
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Marzban
        result = await marzban_worker(redis_client) #type: ignore
        assert result == mock_response, "Marzban success"
    
    # ‚ùå –ò–º–∏—Ç–∞—Ü–∏—è: Redis –ø–∞–¥–∞–µ—Ç –ü–û–°–õ–ï marzban_worker
    # DB –∑–∞–¥–∞—á–∏ –ø–æ—Ç–µ—Ä—è–Ω—ã (–Ω–µ –∑–∞–ø–∏—Å–∞–ª–∏—Å—å –≤ –æ—á–µ—Ä–µ–¥—å –∏–ª–∏ Redis —É–ø–∞–ª)
    await redis_client.flushdb()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è
    db_queue_size = await redis_client.llen("DB") #type: ignore
    assert db_queue_size == 0, "DB queue is empty - tasks lost"
    
    # Marzban —Å–æ–∑–¥–∞–Ω (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏), –Ω–æ –ë–î –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞
    user_repo = BaseRepository(session=test_session, model=User)
    user = await user_repo.get_one(user_id=user_id)
    
    assert user is None, "User NOT in database"
    
    print("\nüí• Critical inconsistency:")
    print(f"   Marzban: ‚úÖ User created")
    print(f"   Database: ‚ùå User missing")
    print(f"   subscription_url: {mock_response['subscription_url']}")
    print(f"   User cannot access - support ticket incoming!")


# ============================================================================
# 4. REDIS MEMORY LIMITS - –¢–µ—Å—Ç—ã –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
# ============================================================================

@pytest.mark.asyncio
async def test_queue_growth_without_limit(redis_client: Redis):
    """
    –¢–µ—Å—Ç: –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Ä–æ—Å—Ç –æ—á–µ—Ä–µ–¥–∏ –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤–æ—Ä–∫–µ—Ä–∞
    
    –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á –∫–æ–≥–¥–∞ –≤–æ—Ä–∫–µ—Ä –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç
    """
    queue = "DB"
    
    # –ò–º–∏—Ç–∞—Ü–∏—è: –≤–æ—Ä–∫–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –∑–∞–¥–∞—á–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç –ø–æ—Å—Ç—É–ø–∞—Ç—å
    for i in range(1000):
        task = {
            "model": "User",
            "type": "create",
            "user_id": 10000 + i
        }
        await redis_client.lpush(queue, json.dumps(task, sort_keys=True)) #type: ignore
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
    queue_size = await redis_client.llen(queue) #type: ignore
    memory_info = await redis_client.info('memory')
    
    used_memory = memory_info.get('used_memory_human', 'unknown')
    
    print(f"\nüìä Queue statistics:")
    print(f"   Size: {queue_size} tasks")
    print(f"   Memory: {used_memory}")
    
    assert queue_size == 1000
    
    # –ï—Å–ª–∏ –∫–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ ~500 –±–∞–π—Ç, 1000 –∑–∞–¥–∞—á = ~500KB
    # –ü—Ä–∏ 1,000,000 –∑–∞–¥–∞—á = ~500MB
    # –ë–µ–∑ maxmemory-policy Redis –º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å!
    
    print(f"\n‚ö†Ô∏è  Without limits, 1M tasks would use ~500MB")
    print(f"   Current: {queue_size} tasks = ~{queue_size * 500 / 1024:.2f}KB")


@pytest.mark.asyncio
async def test_dedup_set_grows_unbounded_without_ttl(redis_client: Redis):
    """
    –¢–µ—Å—Ç: SET –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ —Ä–∞—Å—Ç—ë—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ –±–µ–∑ TTL
    
    –ö–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ SET –∏ –æ—Å—Ç–∞—ë—Ç—Å—è —Ç–∞–º –Ω–∞–≤—Å–µ–≥–¥–∞
    """
    dedup_set = "QUEUE:dedup"
    
    # –î–æ–±–∞–≤–ª—è–µ–º 10,000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
    for i in range(10000):
        task = json.dumps({"user_id": 10000 + i, "type": "create"}, sort_keys=True)
        await redis_client.sadd(dedup_set, task) #type: ignore
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
    set_size = await redis_client.scard(dedup_set) #type: ignore
    
    print(f"\nüìä Dedup SET size: {set_size} items")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
    ttl = await redis_client.ttl(dedup_set)
    print(f"   TTL: {ttl} seconds")
    
    if ttl == -1:
        print(f"\n‚ö†Ô∏è  WARNING: No TTL - set will grow forever!")
        print(f"   Memory leak: each task stays in SET permanently")
        print(f"   After 1M tasks: ~50MB of memory never freed")
    
    assert set_size == 10000


@pytest.mark.asyncio
async def test_memory_leak_from_failed_tasks(redis_client: Redis):
    """
    –¢–µ—Å—Ç: —É—Ç–µ—á–∫–∞ –ø–∞–º—è—Ç–∏ –æ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø–∞–¥–∞—é—â–∏—Ö –∑–∞–¥–∞—á
    
    –ó–∞–¥–∞—á–∞ –ø–∞–¥–∞–µ—Ç ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ –æ—á–µ—Ä–µ–¥—å ‚Üí –ø–∞–¥–∞–µ—Ç —Å–Ω–æ–≤–∞ ‚Üí —Ü–∏–∫–ª
    """
    queue = "DB"
    
    # –ó–∞–¥–∞—á–∞ –∫–æ—Ç–æ—Ä–∞—è –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç –ø–∞–¥–∞—Ç—å
    poison_task = {
        "model": "InvalidModel",  # –ù–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ MODEL_REGISTRY
        "type": "create",
        "user_id": 12345
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º poison task
    await redis_client.lpush(queue, json.dumps(poison_task, sort_keys=True)) #type: ignore
    
    # –ò–º–∏—Ç–∞—Ü–∏—è: –≤–æ—Ä–∫–µ—Ä –ø—ã—Ç–∞–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤ –æ—á–µ—Ä–µ–¥—å
    for attempt in range(100):
        # –ë–µ—Ä—ë–º –∑–∞–¥–∞—á—É
        result = await redis_client.brpop(queue, timeout=1) #type: ignore
        if not result:
            break
        
        _, message = result
        
        # "–û–±—Ä–∞–±–æ—Ç–∫–∞" –ø–∞–¥–∞–µ—Ç
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
        await redis_client.lpush(queue, message) #type: ignore
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–¥–∞—á–∞ –≤—Å—ë –µ—â—ë –≤ –æ—á–µ—Ä–µ–¥–∏
    queue_size = await redis_client.llen(queue) #type: ignore
    assert queue_size == 1, "Poison task still in queue"
    
    print(f"\nüí• Poison task processed {100} times")
    print(f"   Still in queue: {queue_size}")
    print(f"   Without max_retries: infinite loop + log spam")
    
    # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏: –Ω—É–∂–µ–Ω —Å—á—ë—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –∏ Dead Letter Queue


@pytest.mark.asyncio  
async def test_massive_duplicate_detection_memory_spike(redis_client: Redis):
    """
    –¢–µ—Å—Ç: –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ–∑–¥–∞—é—Ç memory spike
    
    100 –≤–æ—Ä–∫–µ—Ä–æ–≤ √ó LRANGE(10000 —ç–ª–µ–º–µ–Ω—Ç–æ–≤) = 1MB √ó 100 = 100MB spike
    """
    queue = "TEST_QUEUE"
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å
    for i in range(10000):
        task = {"user_id": 10000 + i, "type": "test"}
        await redis_client.lpush(queue, json.dumps(task, sort_keys=True)) #type: ignore
    
    # –ò–º–∏—Ç–∞—Ü–∏—è: 100 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
    async def check_duplicate():
        # worker_exsists –¥–µ–ª–∞–µ—Ç LRANGE
        all_items = await redis_client.lrange(queue, 0, -1) #type: ignore
        # –í –ø–∞–º—è—Ç–∏ Python: —Å–ø–∏—Å–æ–∫ –∏–∑ 10000 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ √ó ~500 –±–∞–π—Ç = ~5MB
        return len(all_items)
    
    memory_before = await redis_client.info('memory')
    
    # 100 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
    tasks = [check_duplicate() for _ in range(100)]
    await asyncio.gather(*tasks)
    
    memory_after = await redis_client.info('memory')
    
    print(f"\nüìä Memory spike from duplicate checks:")
    print(f"   Before: {memory_before.get('used_memory_human')}")
    print(f"   After: {memory_after.get('used_memory_human')}")
    print(f"   100 √ó 10K items = 100 √ó 5MB = 500MB potential spike")


# ============================================================================
# SUMMARY TEST - –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
# ============================================================================

@pytest.mark.asyncio
async def test_cascading_failure_scenario(
    redis_client: Redis,
    test_session: AsyncSession
):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç: –∫–∞—Å–∫–∞–¥–Ω—ã–π –æ—Ç–∫–∞–∑ —Å–∏—Å—Ç–µ–º—ã
    
    1. –û—á–µ—Ä–µ–¥—å —Ä–∞—Å—Ç—ë—Ç (–≤–æ—Ä–∫–µ—Ä –º–µ–¥–ª–µ–Ω–Ω—ã–π)
    2. –ü—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–º–µ–¥–ª—è—é—Ç—Å—è
    3. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ retry ‚Üí –µ—â—ë –±–æ–ª—å—à–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    4. Memory spike
    5. Redis OOM
    """
    queue = "TRIAL_ACTIVATION"
    
    # –≠—Ç–∞–ø 1: –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á (–≤–æ—Ä–∫–µ—Ä –º–µ–¥–ª–µ–Ω–Ω—ã–π/–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
    print("\nüìä Stage 1: Queue accumulation")
    for i in range(5000):
        task = {"user_id": 10000 + i, "type": "trial"}
        await redis_client.lpush(queue, json.dumps(task, sort_keys=True)) #type: ignore
    
    queue_size = await redis_client.llen(queue) #type: ignore
    print(f"   Queue size: {queue_size}")
    
    # –≠—Ç–∞–ø 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–º–µ–¥–ª—è–µ—Ç—Å—è
    print(f"\nüìä Stage 2: Duplicate checking slowdown")
    start = time.time()
    test_task = {"user_id": 99999, "type": "trial"}
    await worker_exsists(redis_client, queue, test_task)
    elapsed = time.time() - start
    print(f"   Check time: {elapsed*1000:.2f}ms (would be ~50ms with 1000 items)")
    
    # –≠—Ç–∞–ø 3: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ retry (–¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –¥–µ—Ç–µ–∫—Ç—è—Ç—Å—è –≤–æ–≤—Ä–µ–º—è)
    print(f"\nüìä Stage 3: User retries add duplicates")
    # 100 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π √ó 5 retry = 500 –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    for i in range(100):
        for _ in range(5):
            task = {"user_id": 20000 + i, "type": "trial"}
            await redis_client.lpush(queue, json.dumps(task, sort_keys=True)) #type: ignore
    
    queue_size = await redis_client.llen(queue) #type: ignore
    print(f"   Queue size: {queue_size} (+500 duplicates)")
    
    # –≠—Ç–∞–ø 4: Memory spike –æ—Ç –º–∞—Å—Å–æ–≤—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
    print(f"\nüìä Stage 4: Memory spike from concurrent checks")
    memory_info = await redis_client.info('memory')
    used_memory = memory_info.get('used_memory', 0)
    print(f"   Memory: {memory_info.get('used_memory_human')}")
    
    # –≠—Ç–∞–ø 5: Critical state
    print(f"\nüí• System state: CRITICAL")
    print(f"   Queue: {queue_size} tasks")
    print(f"   Duplicates: ~500 (9%)")
    print(f"   Check latency: {elapsed*1000:.2f}ms")
    print(f"   Memory: {memory_info.get('used_memory_human')}")
    print(f"\n   Without protection:")
    print(f"   - Redis OOM risk")
    print(f"   - User experience degraded")
    print(f"   - Data loss on crash")
    print(f"   - Manual intervention required")
    
    # Assertion: —Å–∏—Å—Ç–µ–º–∞ –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏
    assert queue_size > 5000
    assert elapsed > 0.1  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–º–µ–¥–ª–∏–ª–∞—Å—å


if __name__ == "__main__":
    print("Run with: pytest tests/test_high_priority_scenarios.py -v -s")